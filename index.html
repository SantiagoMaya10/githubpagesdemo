**1. Escoja las dos funciones de prueba:**
Las funciones escogidas para realizar el reporte son:


*   Función de Rosenbrock
*   Función de Rastrigin



A continuación describiremos las funciones de Rastrigin y Rosenbrock en N dimensiones, 2 dimensiones y 3 dimensiones.

Con dimensión se entiende el número de variables que recibe como entrada la función. Además es de resaltar que las funciones que se utlizan en el informe son de la forma $$ f : \mathbb{R}^n \rightarrow  \mathbb{R} $$

Esto es porque, como lo dice Goodfellow et al.: "Para que el concepto de "minimización" tenga sentido tiene que haber solo una salida (escalar)" (2016, p. 82).

# **Función de Rastrigin**

Aqui va la de PAULA


# **Función de Rosenbrock**

### **Características e historia:**
Según Ma & Li, la función de Rosenbrock es una función de utilidad para medir las ventajas y desventajas de algoritmos de optimización sin restricciones y fue propuesta en 1960 por Howard H. Rosenbrock. Es una función no convexa y es usada para mejorar el test de performance del algoritmo de optimización en cuestión ( 2019, p. 110).


### **Principal característica de la función que la hace interesante para benchamrks:**
"Cada contorno de la función de Rosenbrock es apenas parabólica, y el mínimo global de la misma también está en un valle parabólico. Es fácil encontrar este valle, pero bastante difícil encontrar el mínimo de todo el dominio, porque el valor de la función en el valle no cambia mucho" (Ma & Li, 2019, 111).


### **Definición N-dimensional:**

Ma & Li la definen en N dimensiones de la siguiente manera (2019, p. 111):

$$
f(X)= \sum_{i=1}^{N-1}[(1-x_{i})^2+100(x_{i+1}-x_{i}^{2})^2], \forall x \in \mathbb{R}^n
$$



De forma más específica, se puede deducir su expresión para 2 y 3 dimensiones respectivamente.
### **Definición en 2 dimensiones:**
$$f(x_{1},x_{2}) = (1-x_{1})^2 +100(x_{2}-x_{1}^2)^2$$

Se conoce el mínimo global es en $$(x_{1},x_{2})=(1,1)$$

y el valor es es $$f(1,1) = 0$$

### **Definición en 3 dimensiones:**
$$f(x_{1},x_{2}, x_{3}) = [(1-x_{1})^2+100(x_{2}-x_{1}^2)^2]+[(1-x_{2})^2+100(x_{3}-x_{2}^2)^2]$$

Según Ma & Li  se puede demostrar que cuando N=3, la función definida arriba tiene solo un valor mínimo y la posición es $$(x_{1},x_{2},x_{3})=(1,1,1)$$

con un valor de $$f(1,1,1) = 0$$

(2019, p. 111).


Según se muestra en la Figura 1, se observa el mapeo de la función de Rosenbrock de 2 variables, en un espacio tridimensional.

Figura 1
![](https://drive.google.com/uc?export=view&id=1q4-YLysIBrVNO75pX6Ho6mzKJoOlDydr)
Nota: Adaptado de Research on Rosenbrock Function Optimization Problem Based on Improved Differential Evolution Algorithm. Ma, J., & Li, H. (2019)  Journal of Computer and Communications. https://www.scirp.org/pdf/jcc_2019112814542663.pdf. p.111.



References

* Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

*   Ma, J., & Li, H. (2019). Research on Rosenbrock Function Optimization Problem Based on Improved Differential Evolution Algorithm. Journal of Computer and Communications, 7(11), 107-120. 10.4236/jcc.2019.711008

